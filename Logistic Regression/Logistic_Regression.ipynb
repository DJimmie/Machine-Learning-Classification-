{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD THE DEPENDANCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from numpy import set_printoptions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\Crystal\\\\Desktop\\\\Programs\\\\my-modules-and-libraries\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### KNN Classifier\n",
    "\n",
    "def logistic_regression(X_train,y_train,X_test,y_test):\n",
    "    \"\"\"Logistic Regression algorithm\"\"\"\n",
    "    global classifier\n",
    "    \n",
    "    f1_scores=[]\n",
    "    accur=[]\n",
    "    preci=[]\n",
    "    recall=[]\n",
    "    \n",
    "        \n",
    "    # Define Logistic Regression Model\n",
    "    classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                                    C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "                                    random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, \n",
    "                                    verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "    # Fit Model\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    y_prob=classifier.predict_proba(X_test)\n",
    "\n",
    "    y_prob=y_prob[:,1]\n",
    "\n",
    "    f1,a,p,r=metrics(y_test, y_pred)\n",
    "\n",
    "    f1_scores.append(f1)\n",
    "    accur.append(a)\n",
    "    preci.append(p)\n",
    "    recall.append(r)\n",
    "        \n",
    "    print('\\n','f1_scores: ',f1_scores)\n",
    "    print('accuracy: ',accur)\n",
    "    \n",
    "    return f1_scores,accur,preci,recall,y_pred,y_prob\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Evaluate Model\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    \"\"\"Confusion matrix and associated metrics\"\"\"\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn,fp,fn,tp=confusion_matrix(y_test, y_pred).ravel()\n",
    "    precision=precision_score(y_test,y_pred)\n",
    "    recall=recall_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print('Confusion matrix breakdown:',('tn:',tn,'fp:',fp,'fn:',fn,'tp:',tp),'\\n')\n",
    "    print('Confusion matrix:\\n', matrix)\n",
    "    print('Precision: When it predicts yes, how often is it correct?:',precision)\n",
    "    print('Recall.True Positive Rate: When it\\'s actually yes, how often does it predict yes?:',recall)\n",
    "    print('F1:score is the harmonic average of the precision and recall,:',f1)\n",
    "    print('Accuracy.Overall, how often is the classifier correct?: ',accuracy)\n",
    "    print('Misclassification Rate.Overall, how often is it wrong?: ',(1-accuracy))\n",
    "\n",
    "    return (f1,accuracy,precision,recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Replacing zeros where it is not a valid value for that feature.\n",
    "##### This done here by replacing the zero values with a NAN, then replacing the NAN with the average value for non-zero values in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replacing_zeros(dataset,the_headers):\n",
    "    \"\"\"Function used to remove zeros from numeric features when 0 is not practical\"\"\"\n",
    "\n",
    "    for header in the_headers:\n",
    "        dataset[header]=dataset[header].replace(0,np.nan)\n",
    "        mean=int(dataset[header].mean(skipna=True))\n",
    "        dataset[header]=dataset[header].replace(np.nan,mean)\n",
    "        \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_the_dataset(dataset,input_headers,target_header):\n",
    "    \n",
    "#     X=dataset.iloc[:,0,8]\n",
    "#     X=dataset.iloc[:,[1,2,4,5,6,7]]\n",
    "#     y=dataset.iloc[:,8]\n",
    "    X=dataset[input_headers]\n",
    "    y=dataset[target_header]\n",
    "    \n",
    "    X.head()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Quick look at the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def quick_feature_view(X):\n",
    "    \n",
    "\n",
    "    # X.hist(bins=50,figsize=(15,15))\n",
    "    # X.plot(kind='hist',subplots=True,layout=(3,3),sharex=False, figsize=(15,15))\n",
    "\n",
    "    headers=X.columns.tolist()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(headers), figsize=(20, 10))\n",
    "    print(headers)\n",
    "    for i,head in enumerate(headers,0):\n",
    "\n",
    "        axes[i].hist(x=X[head],bins=50,edgecolor='black')\n",
    "        axes[i].set(title=head)\n",
    "        axes[i].grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    X.plot(kind='density',subplots=True,layout=(3,3),sharex=False, figsize=(15,15))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING:Target Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_summary(dataset,target_header):\n",
    "    \"\"\"PREPROCESSING:Target Summary\"\"\"\n",
    "    print(dataset.groupby(target_header).size())\n",
    "    print((dataset.groupby(target_header).size()/len(y)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING:Train - Test Split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_the_train_test_data(X,y):\n",
    "    \n",
    "    \"\"\"PREPROCESSING:Train - Test Split of the data\"\"\"\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=.05,random_state=42,shuffle=True)\n",
    "    X_train.head()\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_scaling(X_train,X_test):\n",
    "    sc_X=StandardScaler()\n",
    "    X_train=sc_X.fit_transform(X=X_train,y=None)\n",
    "    X_test=sc_X.fit_transform(X=X_test,y=None)\n",
    "\n",
    "    print(sc_X.fit(X_train))\n",
    "    print(X_train[0:5])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters:Choose a value of k by taking the sqrt of the number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_k_value(y_test,list_or_single):\n",
    "    \n",
    "    y_test.size\n",
    "    \n",
    "    if (list_or_single.lower()=='s'):\n",
    "        k=round(math.sqrt(y_test.size))\n",
    "        \n",
    "        if (k%2==0):\n",
    "            k_list=[]\n",
    "            k_list.insert(0,k-1)\n",
    "            k_list.insert(1,k+1)\n",
    "            k=k_list\n",
    "    else:\n",
    "        k=[3,5,7,9,11,13,15,17,19,21]\n",
    "        \n",
    "    print ('Selected k value(s):\\n',k)\n",
    "    return k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_the_metrics(f1_scores,accur,preci,recall):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10),sharex='none')\n",
    "    axes[0,0].plot(k,f1_scores,marker='o')\n",
    "    axes[0,1].plot(k,accur,marker='o')\n",
    "    axes[0,0].set(title='F1 Score')\n",
    "    axes[0,1].set(title='Accuracy')\n",
    "    axes[0,0].set(xlabel='K value')\n",
    "    axes[0,1].set(xlabel='K value')\n",
    "#     axes[0,0].set(xlim=(3,21), ylim=(0,1))\n",
    "#     axes[0,1].set(xlim=(3,21), ylim=(0,1))\n",
    "    axes[0,0].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[0,1].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[0,0].grid()\n",
    "    axes[0,1].grid()\n",
    "\n",
    "    axes[1,0].plot(k,preci,marker='o')\n",
    "    axes[1,1].plot(k,recall,marker='o')\n",
    "    axes[1,0].set(title='Precision')\n",
    "    axes[1,1].set(title='Recall')\n",
    "    axes[1,0].set(xlabel='K value')\n",
    "    axes[1,1].set(xlabel='K value')\n",
    "#     axes[1,0].set(xlim=(3,21), ylim=(0,1))\n",
    "#     axes[1,1].set(xlim=(3,21), ylim=(0,1))\n",
    "    axes[1,0].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[1,1].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[1,0].grid()\n",
    "    axes[1,1].grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box_plot_the_metrics(f1_scores,accur,preci,recall):\n",
    "    \"\"\"Box plots for the classification metrics over a range of parameter adjustments\"\"\"\n",
    "    \n",
    "    f=np.asarray(f1_scores)\n",
    "    a=np.asarray(accur)\n",
    "    p=np.asarray(preci)\n",
    "    r=np.asarray(recall)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "    axes.boxplot([f,a,p,r])\n",
    "    axes.set_xticklabels(['f1_score','accuracy','precision','recall'])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_of_data_space(data,labels,input_headers):\n",
    "    \n",
    "    \n",
    "    xx_1=pd.DataFrame(data[:,0]) \n",
    "    xx_2=pd.DataFrame(data[:,1]) \n",
    "    y=pd.DataFrame(labels)\n",
    "\n",
    "#     print(xx_1.head(),'\\n',xx_2.head(),'\\n',y.head())\n",
    "#     print(list(xx_1[y==0]))\n",
    "#     print(list(xx_1[y==1]))\n",
    "    plt.figure(figsize=(15,10)) \n",
    "    plt.scatter(xx_1[y==0],xx_2[y==0],color='b') \n",
    "    plt.scatter(xx_1[y==1],xx_2[y==1],color='r')\n",
    "\n",
    "#     plt.scatter(data[:,0],data[:,1],s=40,c=labels,cmap=plt.cm.Spectral)\n",
    "#     plt.scatter(x_test[:,0],x_test[:,1],color='black')\n",
    "\n",
    "    plt.xlabel(input_headers[0])\n",
    "    plt.ylabel(input_headers[1])\n",
    "#     plt.xlim(xx_1.min(),xx_1.max())\n",
    "#     plt.ylim(xx_2.min(),xx_2.max())\n",
    "    plt.grid() \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boundary_decision_plot(X,y,X_train,y_train,x_test,y_pred,y_prob):\n",
    "    \n",
    "    X_unscaled=X.values\n",
    "    X_scaled, dummy=feature_scaling(X_unscaled,X_test=1)\n",
    "    xx_1=pd.DataFrame(X_train[:,0]) \n",
    "    xx_2=pd.DataFrame(X_train[:,1]) \n",
    "    y=pd.DataFrame(y_train.values)\n",
    "    \n",
    "#     print(y_train[0:5])\n",
    "#     print(X_train[0:5])\n",
    "\n",
    "    xx_test_1=pd.DataFrame(x_test[:,0]) \n",
    "    xx_test_2=pd.DataFrame(x_test[:,1])\n",
    "\n",
    "    y_predict=pd.DataFrame(y_pred) \n",
    "    y_prob=pd.DataFrame(y_prob) \n",
    "\n",
    "    cmap_light = ListedColormap(['#FFAAAA','#AAAAFF'])\n",
    "\n",
    "#     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000','#0000FF'])\n",
    "\n",
    "    h=.02\n",
    "\n",
    "#     Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "#     x1=X_train[:,0] \n",
    "    x1=X_scaled[:,0]\n",
    "    x2=X_scaled[:,1] \n",
    "    x_min,x_max = x1.min()-1,x1.max()+1 \n",
    "    y_min,y_max = x2.min()-1,x2.max()+1 \n",
    "    xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "\n",
    "\n",
    "    the_predict=classifier.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "#     Put the result into a color plot\n",
    "    Z = the_predict.reshape(xx.shape) \n",
    "    plt.figure(figsize=(15,15)) \n",
    "    plt.xlim(xx.min(),xx.max()) \n",
    "    plt.ylim(yy.min(),yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx,yy,Z,cmap=cmap_light)\n",
    "\n",
    "#     plt.scatter(xx_1[y==0],xx_2[y==0],color='b',marker='o') \n",
    "#     plt.scatter(xx_1[y==1],xx_2[y==1],color='r',marker='o')\n",
    "    \n",
    "\n",
    "#     plt.scatter(X_train[:,0],X_train[:,1],s=40,c=y_train,cmap=plt.cm.Spectral)\n",
    "    cm=plt.cm.get_cmap('RdYlBu_r')\n",
    "\n",
    "#     plt.scatter(xx_test_1[y_predict==0],xx_test_2[y_predict==0],cmap=cm,vmin=0,vmax=1,c=y_predict,marker='D') \n",
    "#     plt.scatter(xx_test_1[y_predict==1],xx_test_2[y_predict==1],cmap=cm,vmin=0,vmax=1,c=y_predict,marker='D') \n",
    "    plt.scatter(xx_test_1[y_predict==0],xx_test_2[y_predict==0],cmap=cm,vmin=0,vmax=1,c=y_prob,marker='D') \n",
    "    plt.scatter(xx_test_1[y_predict==1],xx_test_2[y_predict==1],cmap=cm,vmin=0,vmax=1,c=y_prob,marker='D') \n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "    plt.grid() \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_results(f1_scores,accur,preci,recall,k):\n",
    "    \n",
    "    f=np.asarray(f1_scores)\n",
    "    a=np.asarray(accur)\n",
    "    p=np.asarray(preci)\n",
    "    r=np.asarray(recall)\n",
    "    k=np.asarray(k)\n",
    "    \n",
    "    results=f\"\"\"\\n\n",
    "    The max F1 SCORE is {f.max()} with a K value of {k[np.argmax(f)]}\\n\n",
    "    The max ACCURACY is {a.max()} with a K value of {k[np.argmax(a)]}\\n\n",
    "    The max PRECISION is {p.max()} with a K value of {k[np.argmax(p)]}\\n\n",
    "    The max RECALL is {r.max()} with a K value of {k[np.argmax(r)]}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    global classifier\n",
    "    \n",
    "#     RETRIEVE THE DATASET\n",
    "    \n",
    "    location=r'C:\\Users\\Crystal\\Desktop\\Programs\\dataset_repo\\diabetes.csv'\n",
    "#     location=r'C:\\Users\\Crystal\\Desktop\\Programs\\dataset_repo\\CDH_Train.csv'\n",
    "    dataset=pd.read_csv(location)\n",
    "\n",
    "    print(dataset.info())\n",
    "    dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Replace zeros with the mean where needed.\n",
    "    rz=input('Do you need to replace any zeros in the dataset?')\n",
    "    if (rz.lower()=='y'):\n",
    "        the_headers=['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "        dataset=replacing_zeros(dataset,the_headers)\n",
    "        dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Selecting inputs and targets\n",
    "    \n",
    "    target_header=['Outcome']\n",
    "    input_headers=['Glucose','Insulin']\n",
    "#     input_headers=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "\n",
    "#     target_header=['y']\n",
    "#     input_headers=['Length','Width']\n",
    "    \n",
    "    X,y=split_the_dataset(dataset,input_headers,target_header)\n",
    "    if (X.values.shape[1]==2):\n",
    "        plot_of_data_space(X.values,y.values,input_headers)\n",
    "        \n",
    "    print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Splitting the Train-Test data\n",
    "    X_train,X_test,y_train,y_test=split_the_train_test_data(X,y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Scale the data    \n",
    "    X_train, X_test=feature_scaling(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Selection of K\n",
    "    list_or_single=input('(S)ingle or (R)ange of k-values? ')\n",
    "    k=select_k_value(y_test,list_or_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Call the KNN function\n",
    "    f1_scores,accur,preci,recall,y_pred,y_prob=KNN(k,X_train,y_train,X_test,y_test)\n",
    "    print(y_pred)\n",
    "    print(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Plot the metrics\n",
    "    plot_the_metrics(f1_scores,accur,preci,recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #Box plot of the metrics\n",
    "    box_plot_the_metrics(f1_scores,accur,preci,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_results(f1_scores,accur,preci,recall,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Plot the decision boundaries (for input vectors only)\n",
    "    \n",
    "    if (X.values.shape[1]==2):\n",
    "        boundary_decision_plot(X,y,X_train,y_train,X_test,y_pred,y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
