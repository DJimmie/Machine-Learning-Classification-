{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD THE DEPENDANCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from numpy import set_printoptions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\Crystal\\\\Desktop\\\\Programs\\\\my-modules-and-libraries\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### KNN Classifier\n",
    "\n",
    "def logistic_regression(X_train,y_train,X_test,y_test):\n",
    "    \"\"\"Logistic Regression algorithm\"\"\"\n",
    "    global classifier\n",
    "    \n",
    "    f1_scores=[]\n",
    "    accur=[]\n",
    "    preci=[]\n",
    "    recall=[]\n",
    "    \n",
    "        \n",
    "    # Define Logistic Regression Model\n",
    "    classifier = LogisticRegression(penalty='l2', dual=False, tol=0.0001, \n",
    "                                    C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "                                    random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                                    verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "    # Fit Model\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    y_pred=classifier.predict(X_test)\n",
    "\n",
    "    y_prob=classifier.predict_proba(X_test)\n",
    "\n",
    "    y_prob=y_prob[:,1]\n",
    "\n",
    "    f1,a,p,r=metrics(y_test, y_pred)\n",
    "\n",
    "    f1_scores.append(f1)\n",
    "    accur.append(a)\n",
    "    preci.append(p)\n",
    "    recall.append(r)\n",
    "        \n",
    "    print('\\n','f1_scores: ',f1_scores)\n",
    "    print('accuracy: ',accur)\n",
    "    \n",
    "    return f1_scores,accur,preci,recall,y_pred,y_prob\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Evaluate Model\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    \"\"\"Confusion matrix and associated metrics\"\"\"\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "#     print(y_test.shape)\n",
    "\n",
    "    y_test=y_test.values.reshape(y_test.size)\n",
    "#     print(y_test.shape)\n",
    "#     print(y_pred.shape)\n",
    "    \n",
    "    tn,fp,fn,tp=confusion_matrix(y_test,y_pred).ravel()\n",
    "    precision=precision_score(y_test,y_pred)\n",
    "    recall=recall_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    print('Confusion matrix breakdown:',('tn:',tn,'fp:',fp,'fn:',fn,'tp:',tp),'\\n')\n",
    "    print('Confusion matrix:\\n', matrix)\n",
    "    print('Precision: When it predicts yes, how often is it correct?:',precision)\n",
    "    print('Recall.True Positive Rate: When it\\'s actually yes, how often does it predict yes?:',recall)\n",
    "    print('F1:score is the harmonic average of the precision and recall,:',f1)\n",
    "    print('Accuracy.Overall, how often is the classifier correct?: ',accuracy)\n",
    "    print('Misclassification Rate.Overall, how often is it wrong?: ',(1-accuracy))\n",
    "    \n",
    "    plot_confusion_matrix(matrix)\n",
    "\n",
    "    return (f1,accuracy,precision,recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,classes):\n",
    "    fig, ax = plt.subplots()\n",
    "    cmap=plt.cm.Blues\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "    yticks=np.arange(cm.shape[0]),\n",
    "    xticklabels=classes,\n",
    "    yticklabels=classes,\n",
    "    title=\"confusion\",\n",
    "    ylabel='True label',\n",
    "    xlabel='Predicted label')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    normalize=False\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Replacing zeros where it is not a valid value for that feature.\n",
    "##### This done here by replacing the zero values with a NAN, then replacing the NAN with the average value for non-zero values in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replacing_zeros(dataset,the_headers):\n",
    "    \"\"\"Function used to remove zeros from numeric features when 0 is not practical\"\"\"\n",
    "\n",
    "    for header in the_headers:\n",
    "        dataset[header]=dataset[header].replace(0,np.nan)\n",
    "        mean=int(dataset[header].mean(skipna=True))\n",
    "        dataset[header]=dataset[header].replace(np.nan,mean)\n",
    "        \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_the_dataset(dataset,input_headers,target_header):\n",
    "    \n",
    "#     X=dataset.iloc[:,0,8]\n",
    "#     X=dataset.iloc[:,[1,2,4,5,6,7]]\n",
    "#     y=dataset.iloc[:,8]\n",
    "    X=dataset[input_headers]\n",
    "    y=dataset[target_header]\n",
    "    \n",
    "    X.head()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Quick look at the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def quick_feature_view(X):\n",
    "    \n",
    "\n",
    "    # X.hist(bins=50,figsize=(15,15))\n",
    "    # X.plot(kind='hist',subplots=True,layout=(3,3),sharex=False, figsize=(15,15))\n",
    "\n",
    "    headers=X.columns.tolist()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(headers), figsize=(20, 10))\n",
    "    print(headers)\n",
    "    for i,head in enumerate(headers,0):\n",
    "\n",
    "        axes[i].hist(x=X[head],bins=50,edgecolor='black')\n",
    "        axes[i].set(title=head)\n",
    "        axes[i].grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    X.plot(kind='density',subplots=True,layout=(3,3),sharex=False, figsize=(15,15))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING:Target Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_summary(dataset,target_header):\n",
    "    \"\"\"PREPROCESSING:Target Summary\"\"\"\n",
    "    print(dataset.groupby(target_header).size())\n",
    "    print((dataset.groupby(target_header).size()/len(y)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING:Train - Test Split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_the_train_test_data(X,y):\n",
    "    \n",
    "    \"\"\"PREPROCESSING:Train - Test Split of the data\"\"\"\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=.30,random_state=42,shuffle=True)\n",
    "    X_train.head()\n",
    "    \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_scaling(X_train,X_test):\n",
    "    sc_X=StandardScaler()\n",
    "    X_train=sc_X.fit_transform(X=X_train,y=None)\n",
    "    X_test=sc_X.fit_transform(X=X_test,y=None)\n",
    "\n",
    "    print(sc_X.fit(X_train))\n",
    "    print(X_train[0:5])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_the_metrics(f1_scores,accur,preci,recall):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10),sharex='none')\n",
    "    axes[0,0].plot(k,f1_scores,marker='o')\n",
    "    axes[0,1].plot(k,accur,marker='o')\n",
    "    axes[0,0].set(title='F1 Score')\n",
    "    axes[0,1].set(title='Accuracy')\n",
    "    axes[0,0].set(xlabel='K value')\n",
    "    axes[0,1].set(xlabel='K value')\n",
    "#     axes[0,0].set(xlim=(3,21), ylim=(0,1))\n",
    "#     axes[0,1].set(xlim=(3,21), ylim=(0,1))\n",
    "    axes[0,0].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[0,1].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[0,0].grid()\n",
    "    axes[0,1].grid()\n",
    "\n",
    "    axes[1,0].plot(k,preci,marker='o')\n",
    "    axes[1,1].plot(k,recall,marker='o')\n",
    "    axes[1,0].set(title='Precision')\n",
    "    axes[1,1].set(title='Recall')\n",
    "    axes[1,0].set(xlabel='K value')\n",
    "    axes[1,1].set(xlabel='K value')\n",
    "#     axes[1,0].set(xlim=(3,21), ylim=(0,1))\n",
    "#     axes[1,1].set(xlim=(3,21), ylim=(0,1))\n",
    "    axes[1,0].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[1,1].set(xticks=range(3,23,2),yticks=np.arange(0,1.1,.1))\n",
    "    axes[1,0].grid()\n",
    "    axes[1,1].grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def box_plot_the_metrics(f1_scores,accur,preci,recall):\n",
    "    \"\"\"Box plots for the classification metrics over a range of parameter adjustments\"\"\"\n",
    "    \n",
    "    f=np.asarray(f1_scores)\n",
    "    a=np.asarray(accur)\n",
    "    p=np.asarray(preci)\n",
    "    r=np.asarray(recall)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "    axes.boxplot([f,a,p,r])\n",
    "    axes.set_xticklabels(['f1_score','accuracy','precision','recall'])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_of_data_space(data,labels,input_headers):\n",
    "    \n",
    "    \n",
    "    xx_1=pd.DataFrame(data[:,0]) \n",
    "    xx_2=pd.DataFrame(data[:,1]) \n",
    "    y=pd.DataFrame(labels)\n",
    "\n",
    "#     print(xx_1.head(),'\\n',xx_2.head(),'\\n',y.head())\n",
    "#     print(list(xx_1[y==0]))\n",
    "#     print(list(xx_1[y==1]))\n",
    "    plt.figure(figsize=(15,10)) \n",
    "    b=plt.scatter(xx_1[y==0],xx_2[y==0],color='b') \n",
    "    r=plt.scatter(xx_1[y==1],xx_2[y==1],color='r')\n",
    "    g=plt.scatter(xx_1[y==2],xx_2[y==2],color='g')\n",
    "\n",
    "#     plt.scatter(data[:,0],data[:,1],s=40,c=labels,cmap=plt.cm.Spectral)\n",
    "#     plt.scatter(x_test[:,0],x_test[:,1],color='black')\n",
    "\n",
    "    plt.xlabel(input_headers[0])\n",
    "    plt.ylabel(input_headers[1])\n",
    "#     plt.xlim(xx_1.min(),xx_1.max())\n",
    "#     plt.ylim(xx_2.min(),xx_2.max())\n",
    "    plt.grid()\n",
    "    plt.legend((b,r,g),tuple(np.unique(labels)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boundary_decision_plot(X,y,X_train,y_train,x_test,y_pred,y_prob):\n",
    "    \n",
    "    X_unscaled=X.values\n",
    "    X_scaled, dummy=feature_scaling(X_unscaled,X_test=1)\n",
    "    xx_1=pd.DataFrame(X_train[:,0]) \n",
    "    xx_2=pd.DataFrame(X_train[:,1]) \n",
    "    y=pd.DataFrame(y_train.values)\n",
    "    \n",
    "#     print(y_train[0:5])\n",
    "#     print(X_train[0:5])\n",
    "\n",
    "    xx_test_1=pd.DataFrame(x_test[:,0]) \n",
    "    xx_test_2=pd.DataFrame(x_test[:,1])\n",
    "\n",
    "    y_predict=pd.DataFrame(y_pred) \n",
    "    y_prob=pd.DataFrame(y_prob) \n",
    "\n",
    "    cmap_light = ListedColormap(['#FFAAAA','#AAAAFF'])\n",
    "\n",
    "#     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000','#0000FF'])\n",
    "\n",
    "    h=.02\n",
    "\n",
    "#     Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "#     x1=X_train[:,0] \n",
    "    x1=X_scaled[:,0]\n",
    "    x2=X_scaled[:,1] \n",
    "    x_min,x_max = x1.min()-1,x1.max()+1 \n",
    "    y_min,y_max = x2.min()-1,x2.max()+1 \n",
    "    xx,yy = np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h))\n",
    "\n",
    "\n",
    "    the_predict=classifier.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "#     Put the result into a color plot\n",
    "    Z = the_predict.reshape(xx.shape) \n",
    "    plt.figure(figsize=(15,15)) \n",
    "    plt.xlim(xx.min(),xx.max()) \n",
    "    plt.ylim(yy.min(),yy.max())\n",
    "\n",
    "    plt.pcolormesh(xx,yy,Z,cmap=cmap_light)\n",
    "\n",
    "#     plt.scatter(xx_1[y==0],xx_2[y==0],color='b',marker='o') \n",
    "#     plt.scatter(xx_1[y==1],xx_2[y==1],color='r',marker='o')\n",
    "    \n",
    "\n",
    "#     plt.scatter(X_train[:,0],X_train[:,1],s=40,c=y_train,cmap=plt.cm.Spectral)\n",
    "    cm=plt.cm.get_cmap('RdYlBu_r')\n",
    "\n",
    "#     plt.scatter(xx_test_1[y_predict==0],xx_test_2[y_predict==0],cmap=cm,vmin=0,vmax=1,c=y_predict,marker='D') \n",
    "#     plt.scatter(xx_test_1[y_predict==1],xx_test_2[y_predict==1],cmap=cm,vmin=0,vmax=1,c=y_predict,marker='D') \n",
    "    plt.scatter(xx_test_1[y_predict==0],xx_test_2[y_predict==0],cmap=cm,vmin=0,vmax=1,c=y_prob,marker='D') \n",
    "    plt.scatter(xx_test_1[y_predict==1],xx_test_2[y_predict==1],cmap=cm,vmin=0,vmax=1,c=y_prob,marker='D') \n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "    plt.grid() \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_results(f1_scores,accur,preci,recall):\n",
    "    \n",
    "    f=np.asarray(f1_scores)\n",
    "    a=np.asarray(accur)\n",
    "    p=np.asarray(preci)\n",
    "    r=np.asarray(recall)\n",
    "    \n",
    "    \n",
    "    results=f\"\"\"\\n\n",
    "    The max F1 SCORE is {round((f.max()*100),1)}% \n",
    "    The max ACCURACY is {round((a.max()*100),1)}% \n",
    "    The max PRECISION is {round((p.max()*100),1)}% \n",
    "    The max RECALL is {round((r.max()*100),1)}%\n",
    "    \"\"\"\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encoding(dataset,input_headers):\n",
    "    \n",
    "    for i in input_headers:\n",
    "        \n",
    "        the_data_type=dataset[i].dtype.name\n",
    "        if (the_data_type=='object'):\n",
    "            lable_enc=preprocessing.LabelEncoder()\n",
    "            lable_enc.fit(dataset[i])\n",
    "            labels=lable_enc.classes_   #this is an array\n",
    "            labels=list(labels) #converting the labels array to a list\n",
    "            print(labels)\n",
    "            dataset[i]=lable_enc.transform(dataset[i])\n",
    "#             print(dataset[i])\n",
    "\n",
    "    return dataset[i],labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_zeros(dataset,header):\n",
    "    # Replace zeros with the mean where needed.\n",
    "    rz=input('Do you need to replace any zeros in the dataset?')\n",
    "    if (rz.lower()=='y'):\n",
    "        the_headers=[header]\n",
    "        dataset=replacing_zeros(dataset,the_headers)\n",
    "        dataset.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7109 entries, 0 to 7108\n",
      "Data columns (total 43 columns):\n",
      "play_id                      7109 non-null int64\n",
      "game_id                      7109 non-null int64\n",
      "home_team                    7109 non-null object\n",
      "away_team                    7109 non-null object\n",
      "posteam                      7109 non-null object\n",
      "posteam_type                 7109 non-null object\n",
      "defteam                      7109 non-null object\n",
      "side_of_field                7109 non-null object\n",
      "yardline_100                 7109 non-null float64\n",
      "game_date                    7109 non-null object\n",
      "quarter_seconds_remaining    7109 non-null float64\n",
      "half_seconds_remaining       7109 non-null float64\n",
      "game_seconds_remaining       7109 non-null float64\n",
      "game_half                    7109 non-null object\n",
      "quarter_end                  7109 non-null int64\n",
      "drive                        7109 non-null int64\n",
      "sp                           7109 non-null int64\n",
      "qtr                          7109 non-null int64\n",
      "down                         7096 non-null float64\n",
      "goal_to_go                   7109 non-null float64\n",
      "time                         7109 non-null object\n",
      "yrdln                        7109 non-null object\n",
      "ydstogo                      7109 non-null int64\n",
      "ydsnet                       7109 non-null int64\n",
      "desc                         7109 non-null object\n",
      "play_type                    7109 non-null object\n",
      "yards_gained                 7109 non-null int64\n",
      "shotgun                      7109 non-null int64\n",
      "no_huddle                    7109 non-null int64\n",
      "pass_length                  3825 non-null object\n",
      "pass_location                3825 non-null object\n",
      "air_yards                    3834 non-null float64\n",
      "yards_after_catch            2353 non-null float64\n",
      "run_location                 3000 non-null object\n",
      "run_gap                      2164 non-null object\n",
      "field_goal_result            0 non-null float64\n",
      "score_differential           7109 non-null float64\n",
      "score_differential_post      7109 non-null float64\n",
      "rush_attempt                 7109 non-null float64\n",
      "pass_attempt                 7109 non-null float64\n",
      "touchdown                    7109 non-null float64\n",
      "pass_touchdown               7109 non-null float64\n",
      "rush_touchdown               7109 non-null float64\n",
      "dtypes: float64(16), int64(11), object(16)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    global classifier\n",
    "    \n",
    "#     RETRIEVE THE DATASET\n",
    "    \n",
    "    location=r'C:/Users/Crystal/Desktop/Programs/dataset_repo/ml_NF/ml_NFL_pass_run.csv'\n",
    "#     location=r'C:\\Users\\Crystal\\Desktop\\Programs\\dataset_repo\\diabetes.csv'\n",
    "#     location=r'C:\\Users\\Crystal\\Desktop\\Programs\\dataset_repo\\CDH_Train.csv'\n",
    "    dataset=pd.read_csv(location)\n",
    "#     dataset=pd.read_pickle(location)\n",
    "\n",
    "    print(dataset.info())\n",
    "    dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     replace_zeros(dataset,header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pass', 'run']\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'labels' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-3001907ddccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print('target label:',target_label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#     print('test label:',test_label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_the_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_headers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_header\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-256-17a24428ea86>\u001b[0m in \u001b[0;36mlabel_encoding\u001b[1;34m(dataset, input_headers)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#             print(dataset[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'labels' referenced before assignment"
     ]
    }
   ],
   "source": [
    "    #Selecting inputs and targets\n",
    "    \n",
    "    target_header=['play_type']\n",
    "    input_headers=['score_differential','game_seconds_remaining','yardline_100','qtr']\n",
    "    \n",
    "    target_label=label_encoding(dataset,target_header)\n",
    "    \n",
    "#     print('target label:',target_label)\n",
    "    test_label=label_encoding(dataset,input_headers)\n",
    "#     print('test label:',test_label)\n",
    "    X,y=split_the_dataset(dataset,input_headers,target_header)\n",
    "    if (X.values.shape[1]==2):\n",
    "#         print(y.values)\n",
    "        plot_of_data_space(X.values,y.values,input_headers)\n",
    "        \n",
    "#     print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "5       1\n",
       "6       0\n",
       "7       0\n",
       "8       1\n",
       "9       0\n",
       "10      0\n",
       "11      1\n",
       "12      0\n",
       "13      1\n",
       "14      0\n",
       "15      0\n",
       "16      1\n",
       "17      1\n",
       "18      0\n",
       "19      0\n",
       "20      0\n",
       "21      1\n",
       "22      0\n",
       "23      0\n",
       "24      0\n",
       "25      1\n",
       "26      1\n",
       "27      0\n",
       "28      1\n",
       "29      0\n",
       "       ..\n",
       "7079    0\n",
       "7080    0\n",
       "7081    1\n",
       "7082    1\n",
       "7083    0\n",
       "7084    0\n",
       "7085    0\n",
       "7086    1\n",
       "7087    1\n",
       "7088    0\n",
       "7089    0\n",
       "7090    0\n",
       "7091    1\n",
       "7092    0\n",
       "7093    0\n",
       "7094    1\n",
       "7095    1\n",
       "7096    1\n",
       "7097    0\n",
       "7098    1\n",
       "7099    1\n",
       "7100    0\n",
       "7101    0\n",
       "7102    1\n",
       "7103    0\n",
       "7104    0\n",
       "7105    1\n",
       "7106    0\n",
       "7107    1\n",
       "7108    1\n",
       "Name: play_type, Length: 7109, dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133, 1)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #Splitting the Train-Test data\n",
    "    X_train,X_test,y_train,y_test=split_the_train_test_data(X,y)\n",
    "    y_test.shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "[[ 0.11263205 -0.70142389 -1.26393251  0.40055421]\n",
      " [ 0.30842492 -1.15008369  0.409369    1.28248243]\n",
      " [ 0.11263205  1.04283616 -1.67205483 -1.36330222]\n",
      " [-0.572643   -0.45998408 -0.65174903  0.40055421]\n",
      " [ 0.60211423 -1.18145185  0.85830355  1.28248243]]\n"
     ]
    }
   ],
   "source": [
    "    #Scale the data    \n",
    "    X_train, X_test=feature_scaling(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix breakdown: ('tn:', 1014, 'fp:', 219, 'fn:', 654, 'tp:', 246) \n",
      "\n",
      "Confusion matrix:\n",
      " [[1014  219]\n",
      " [ 654  246]]\n",
      "Precision: When it predicts yes, how often is it correct?: 0.5290322580645161\n",
      "Recall.True Positive Rate: When it's actually yes, how often does it predict yes?: 0.2733333333333333\n",
      "F1:score is the harmonic average of the precision and recall,: 0.3604395604395604\n",
      "Accuracy.Overall, how often is the classifier correct?:  0.5907172995780591\n",
      "Misclassification Rate.Overall, how often is it wrong?:  0.4092827004219409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Programming\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Public\\Programming\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = None.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_confusion_matrix() missing 1 required positional argument: 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-d10de5976432>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Call the Logistic Regression function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf1_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccur\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreci\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-3884ba612c7e>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0my_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mf1_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-219-aca8d292cce5>\u001b[0m in \u001b[0;36mmetrics\u001b[1;34m(y_test, y_pred)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Misclassification Rate.Overall, how often is it wrong?: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_confusion_matrix() missing 1 required positional argument: 'classes'"
     ]
    }
   ],
   "source": [
    "    #Call the Logistic Regression function\n",
    "    f1_scores,accur,preci,recall,y_pred,y_prob=logistic_regression(X_train,y_train,X_test,y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Plot the metrics\n",
    "#     plot_the_metrics(f1_scores,accur,preci,recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Box plot of the metrics\n",
    "    box_plot_the_metrics(f1_scores,accur,preci,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    max_results(f1_scores,accur,preci,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #Plot the decision boundaries (for input vectors only)\n",
    "    \n",
    "    if (X.values.shape[1]==2):\n",
    "        boundary_decision_plot(X,y,X_train,y_train,X_test,y_pred,y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.down.isna()].play_type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
